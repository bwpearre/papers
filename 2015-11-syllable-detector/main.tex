\documentclass{article}

\title{A fast and accurate zebra finch syllable detector}
\author{Ben Pearre, L. Nathan Perkins, Jeffrey E. Markowitz, Timothy J. Gardner}

\usepackage{amsmath, amsthm, amssymb, wasysym, graphicx}
\usepackage[small, hang, bf]{caption}
\usepackage{natbib}
\renewcommand\cite{\citep}
\newcommand\citepossessive[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand\eq[1]{Equation~\ref{#1}}
\newcommand\fig[1]{Figure~\ref{#1}}


\setlength{\marginparwidth}{35mm}
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]
  {\raggedright\footnotesize #1}}


\newcommand\argmin{\mathop{\mbox{{\rm argmin}}}\limits}
\newcommand{\noprint}[1]{}


  

\begin{document}
\maketitle

\begin{abstract}
  We present an accurate, versatile, and fast syllable detector that
  can control hardware at precisely timed moments during zebra finch
  song. Most moments during song can be isolated and detected with
  $>95$\% accuracy, easier syllables exceed 99.5\% detection, and
  false positive rates can be $<1\%$\marginpar{How to explain how to
    quantify this in the abstract?}. The detector can run on a stock
  Mac Mini with a triggering delay around 1 millisecond and trigger
  jitter with $\sigma\approx 3$ milliseconds.
\end{abstract}

\section{Introduction}

The adult zebra finch sings a song made up of 2-6 syllables, with
longer songs taking on the order of a second. The song may be repeated
hundreds of times per day, and is almost identical each time. This
consistency presents a unique opportunity to study the neural basis of
learning, audition, and control: we do not know of another model in
which \marginpar{Quantify? Remove vacuous handwaving? Figure?} neural
activity may be so easily correlated with motor output.

In order to take advantage of this consistency, it is useful to be
able to detect selected moments during song, in order to align data or
trigger\marginpar{Examples?} other systems.  To the best of our
knowledge, current systems require expensive hardware, extensive
hand-tuning, and careful choice of an easy syllable.

We developed a standalone detector based on the song
spectrogram. Given a set of aligned songs, our software computes
spectrograms and trains a neural network to output a TTL pulse at the
chosen moment. The approach consists of three steps:

\begin{enumerate}
\item Align songs. This is outside the scope of this paper, but we
  provide a pointer to our song alignment software in
  Section~\ref{sec:resources}.
\item Choose one or more desired trigger
  syllables\marginpar{Terminology: ``syllable'' is a unit, but this
    detector triggers on ``song moments'' or something?}, and train a
  neural network to recognise them. This is carried out offline using
  any recent version of Matlab, and we recommend a computer with at
  least 32GB of RAM.
\item Once trained, the Matlab output file is used by a realtime
  detection program written in Swift, which uses the Mac's stereo
  audio jack and the Core Audio interface. Detection of the target
  syllables is indicated by a TTL pulse. Hardware requirements for this
  system are minimal: we use the current (2015) Mac Mini, which is
  overkill.
\end{enumerate}

We present the method in Section~\ref{sec:method}.
Section~\ref{sec:quantify} describes how we define and test
performance.  Section~\ref{sec:results} presents our measurements.
Section~\ref{sec:case} gives an example usage case of the detector. We
conclude in Section~\ref{sec:conclusion}, and point to software
resources in Appendix~\ref{sec:resources}.


\section{Method}
\label{sec:method}

\subsection{Learning a detector}

We begin with a few hundred time-aligned recordings of a given bird's
song.

One or more moments during the song must be chosen.  Our interface
presents the time-aligned spectrogram and requires manual input of the
target times. The user may next tune the region in frequency and time
used for syllable identification. Then we assemble the training set
from the song data, train the network, compute optimal output unit
thresholds, and save the
network file and an audio test file.


\subsubsection{Recognition region}

The neural network uses several concurrent timesteps from the song
spectrogram.  The range of frequencies $F$ and length of the history
$T$ of this recognition region should be chosen in order to contain
unique features of the target syllable and surrounding areas. This
process could perhaps be automated, but we have not done so, as
hand-choosing is neither difficult nor particularly error-prone.

\subsubsection{Song micro-realignment}

As conditions change, and especially during undirected song, syllable
length and relative timing may vary slightly, which introduces
variations in the precise timing of each syllable. The alignment
software we use ensures that songs are aligned at the point midway
through the song, but if the target syllable is not at that point, it
is helpful to re-align the songs at the point of interest.  This may
be accomplished by looking for peaks in the correlation of the
time-domain signal with the song whose spectrogram is closest to
average over the training set.\marginpar{I hope that's the right thing
  to do. Seems to work, anyway\dots}

\subsubsection{Normalisation}
Jeff and Nathan changed this. What's our current state of the art?\marginpar{FIXME}

\subsubsection{Building the training set}

The neural network's training set is created in the typical fashion:
the rectangular $F\times T$ recognition region in the spectrogram is
simply reshaped into a vector of length $FT$. Each possible region of
size $F\times T$ is converted into a training input vector.

With inputs well outside the space on which a neural network has been
trained, its outputs will be essentially random. In order to reduce
the false negative rate it is necessary to provide negative training
examples that include silence, cage noise, non-song vocalisations, and
perhaps songs from other birds. We have found that training with
roughly a 8:1 ratio of non-song to song yields excellent results,
although this will depend on the makeup of the non-song data.

Training targets are, roughly, 1 if the input vector comes from the
target time, 0 otherwise, for each target syllable. Since the song
realignment may not be perfect, due to sample aliasing, and because
the zebra finch's song seems not to vary faster than this, this output
vector may be spread in time, such that at the target moment it is 1,
and at neighbouring moments it is nonzero. We found that a Gaussian
smoothing kernel around the target time with $\sigma\simeq 3$ms serves
well.

\subsubsection{Training the network}

The network is trained using Matlab's neural network toolbox. We tried
a variety of feedforward neural network geometries, from simple
1-layer perceptrons to more complicated forms and many hidden
nodes. Perhaps surprisingly, even the former yields excellent results
on many syllables, but a 2-layer perceptron with a very small hidden
layer---just 1 or 2 more units than the number of target
syllables---was a good compromise between accuracy and training
speed. Various other neural network geometries could be tried, as well
as any other classifier that executes quickly.  For more variable
songs, deep structure-preserving networks may be more appropriate, but
they are slow to train and unnecessary for zebra finch song.

Matlab's neural network toolbox defaults to Levenburg-Marquardt
training. This is a fast algorithm, but is memory-intensive, so
multiple output syllables or high FFT frame rates require a large
amout of RAM and increase training time to hours. Other training
algorithms that use less RAM are much slower, and by default they will
often terminate before converging due to their performance gradient
going to 0.

\subsubsection{Computing optimal output thresholds}
\label{sec:optimalthresholds}
When the network is trained, outputs of the classifier for any input
are now available, and will be in the \marginpar{Or is it (-1, 1)?}
interval (0, 1). We must choose a threshold above which the output is
considered a positive detection. Finding the optimal threshold
requires two choices. The first is the relative cost of false
negatives to false positives, $C$. The second is the acceptable time
interval: if the true event occurs at time $t$, and the detector
triggers at any time $t\pm\Delta t$, then it is considered a correct
detection. Then the optimal detection threshold $\tau$ is the one that
minimises $\mbox{[false positives]} +C\cdot\mbox{[false negatives]}$
over the training set, using the definitions of false positives and
negatives given in Section~\ref{sec:accuracy}. Since large
portions of the cost function are flat, random-restart hillclimbing
would be effective, but a brute-force search requires fractions of a
second. For the results presented here, we have used $C=1$ and $\Delta
t=20$ms.\marginpar{I chose this large value for $\Delta t$ before I
  did target syllable realignment, and it could probably be much
  reduced now. Does this number make our results look bad? Should I
  explain it, change it and rerun, etc?}

\subsubsection{Our parameter choices}

We use a size-256 FFT, a Hamming window, and a spectrum sampled every
2 milliseconds.\marginpar{Check with Nathan and Jeff on what they've
  used.}  We usually define the network's input space to be 20-80ms
long, and to span frequencies from about 1.5-7kHz, which contains the
fundamentals and several overtones of most zebra finch vocalisations.

We found these parameters to work well across a variety of target
syllables, but various parameter sets yield results similar to those
presented here.  Some of the parameters trade off detection accuracy
or detection timing vs.~training time. For example, detection latency
and jitter cannot be less than the FFT frame rate, but reducing the
frame rate increases the size of the training set and thus the
training resources required (and also increases the computational
burden on the realtime detector, but that seems low to begin
with).


\subsection{Realtime detection}

ipsum lorum\marginpar{Nathan?}

\section{Quantification}
\label{sec:quantify}


Ground truth is given on the training set, and can be measured by
presenting the recorded training songs as well as the canonical
detection events. To this end, besides the trained network object, our
Matlab code produces an audio file consisting of all of the training
data on the left audio channel and delta functions at the moment of
correct detection (basically a TTL pulse, although the voltage will
fluctuate with audio output volume) on the right channel. Thus, when
played on any audio player, the left channel may be provided as input
to the Swift detector, and the right channel may be compared against
the Swift detector's detection pulse.


\subsection{Accuracy}
\label{sec:accuracy}

The Matlab neural network toolbox breaks the given training set into
three groups: data on which the network is trained, data used to
validate the progress of the training algorithm, and holdout test
data\marginpar{It's a little more complex than that
  (cross-validation), but can we go with this?} used only as a final
measure of performance. We further withold a portion of the training
data in order to provide another evaluation of performance on unseen
data.

We define accuracy on a per-song basis, as follows:

\begin{description}
  \item[True positive:] A song for which the detector fires within
    20ms of the intended moment.
  \item[True negative:] So that every non-detected non-syllable does
    not count, a true negative is a complete song during which the
    detector did not fire at any point outside the true positive
    region.
  \item[False positive:] A detection event more than 20ms from the
    target syllable.
  \item[False negative:] A song's target interval during which a
    detection event should have occurred, but didn't.
\end{description}

These definitions are used for computing the optimal thresholds above
which the network's output should be interpreted as a match on the
training data as described in Section~\ref{sec:optimalthresholds}, for
evaluation of the detector on the training songs in Matlab and Swift,
and while live.

Given those definitions, we can compute the false-positive and\marginpar{Would be good to measure FP and FN rates on Nathan's detector! With the test audio file, it's easy---see the LabView code for my spaghetti implementation, or do it anew.} false-negative rates. For simplicity, we present detector accuracy using the area under the ROC curve.

\noprint{
Since the network
will output values $o_t$ between 0 and 1 at each moment $t$ in an
attempt to match the training output, the optimal threshold
$\tau\in[0,1]$ for the output neuron should be computed.  Given the
relative cost of false positives vs.~false negatives $C$, and the
accptable time difference between target syllable and correct output
$\Delta t_d$, we compute the optimal threshold for an output element
according to the definitions above:
\begin{eqnarray*}
  \mbox{true positives}_\tau &=& \mbox{size of set}_{s\in \mbox{target songs}} o_t > \tau, \left| t \leq \Delta t_d \right| \\
  \mbox{false negatives}_\tau &=& \mbox{size of set} {s\in\mbox{target songs}} - \mbox{size of set} \mbox{true positives} \\
  \mbox{false positives}_\tau &=& \mbox{size of set}_{s\in \mbox{target songs}} o_t > \tau, \left| t > \Delta t_d \right| \\
  \widehat{\tau} &=& \argmin_\tau C\mbox{false positive} + \mbox{false negatives}
\end{eqnarray*}
}

\subsection{Timing}

We evaluate the time taken from the presentation of the target
syllable to the firing of the detector's TTL pulse. While playing the
audio test file from another device (such as a mobile phone), the TTL
output from the ground-truth channel of the audio output may be used
as the trigger pulse for an oscilloscope, and compared to the TTL pulse
produced by the Swift detector, which sees only the birdsong channel
of the audio file. For this purpose we used a pulse
generator \marginpar{Model? Ramp time? Latency?} to widen the
detector's output spike to about 100ms and set the oscilloscope to
averaging mode. The canonical signal is the trigger at $t=0$, and the
average of the detector's detection events will be seen as a
low-to-high transition with form approximating the cumulative
probability distribution function (CDF) of the detector's output in
response to the chosen song event.

Mean latency is then given as the halfway point of that detection
sigmoid. It is a helpful number, but not a critical one, since a
detector with high but constent latency can be trained to trigger at a
point somewhat before the true moment of interest.

Often more important is latency jitter: how much random variability is
there in the latency? This we obtain by eyeballing the width of the
detection sigmoid in the oscilloscope.


\section{Results}
\label{sec:results}

\marginpar{Bases for comparison? Jeff's matched filterbank? Nothing really
informative.}

\fig{fig:sixsyllables} shows a typical song, with six target moments
selected.  We trained the network with 8 hidden units for the 6
detection targets spaced 10ms apart.  Matlab's self-report of
detection \marginpar{I should generate, say, 100 random detection
  points in a few songs, so I can say what something like ``average
  accuracy'' is.}  performance for each iteration of the song is shown
in \fig{fig:sixsyllables_out}, with ROC curves shown in \fig{fig:roc}.
The beginning of the syllable is difficult to detect, with a few
detection events considerably earlier than the correct moment.  But
the syllable quickly becomes reliably identifiable.  By the time the
detector has seen 50ms of the syllable---the sixth detection
point---performance is good: the area under the ROC curve is 0.992.

\fig{fig:timing_nathan} shows the realtime Swift detector running on a
single syllable from another bird, using the test audio file generated
during training on that song (not shown).  The oscilloscope is
triggering on the true signal on CH2 (blue), while CH1 (yellow) shows
the average of the detection events, using a pulse generator as
described above (a single detection event, non-averaged, would be a
single low-to-high step function).  Each grid square is 10ms long, so
average detection latency appears to be around 1ms, while jitter
appears to have $\sigma\simeq 3$ms or so (eyeballed).

\fig{fig:timing_ben}\marginpar{Figure this out! Compare on the same
  syllable, at least!} is similar to \fig{fig:timing_nathan} but
tested using the LabView implementation, on a different syllable from
a different bird, with different parameters.  Latency is much higher
($\simeq 11$ms, but latency jitter is more like $\sigma\simeq
1.5$ms---note that the grid spacing is 2.5ms.

\fig{fig:bwpvstdt} could show\marginpar{FIXME} a comparison between Jeff's filtering approach on the TDT and my FFT-NN. But how much time do I want to spend describing Jeff's filters? Not much, since it's not in use anywhere besides our lab. Or I could simply insert a photo of Jeff, holding my FFT-NN and a finch, and smiling, with ``My name is Dr.~Jeffrey~E.~Markowitz and I endorse this software!''

\begin{figure}
  \includegraphics[width=\textwidth]{6_syllables}
  \caption{A spectrogram made by averaging over 526 songs. The 6
    target syllables, spaced 10ms apart, are marked by red lines.}
    \label{fig:sixsyllables}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{6_syllables_out}
    \caption{Detection results. Each plot shows detection events for
      the corresponding syllable as shown in
      \fig{fig:sixsyllables}. The horizontal axis is time, and the
      vertical axis is the index of the 526 single song
      presentations. The grey shading shows the total audio energy of
      song Y at time X. The bar on the left is the same width as the
      detection window, so no detection events can happen within that
      region, and its colour code shows training songs to the right of
      red regions and unseen test songs to the right of cyan
      regions. For visualisation of the distribution, songs have been
      stably sorted by the time of detection events.}
  \label{fig:sixsyllables_out}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{6_syllables_roc}
  \caption{ROC curves for the detection of targets shown in Figure~\ref{fig:sixsyllables}. The first one, at the beginning of a syllable, is the most difficult to detect, and as more structure emerges in the current syllable, accuracy approaches 100\%.}
  \label{fig:roc}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{timing_nathan}
  \caption{Timing curve for a syllable running a test file generated by training.}
  \label{fig:timing_nathan}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{detector-timing-2}
  \caption{Alternate view of timing, from the LabView implementation. Seems better than that in \fig{fig:timing_nathan}, doesn't it? Hmmm\dots have to discuss with Nathan!}
  \label{fig:timing_ben}
\end{figure}

\section{Case Study?}
\label{sec:case}

Jeff is using the detector for something cool, and it might be fun to get him to describe it in a paragraph or two.\marginpar{Do we want to include a case study? I think the paper is on the long side already, but it might be interesting and let Jeff earn that coauthorship ;)}

Failing that, I can immediately do song-aligned X spike rasters if lw95rhp is still willing to sing\dots but I have not heard him do so lately.

\section{Conclusion}
\label{sec:conclusion}

This syllable detector is appropriate for zebra finch song.  It offers
the following benefits:
\begin{itemize}
\item False positive and false negative rates can be well under 1\%.
\item Latency and jitter are both in the range of 2 milliseconds.
\item Works on a wide range of target syllables.
\item Requires minimal hand-tuning.
\item Runs in realtime on inexpensive consumer-grade hardware---we use the Mac Mini with 8GB RAM.
\item We recommend training the network on a computer with at least 32GB of RAM.
\end{itemize}
It has not been evaluated in other species, and the high accuracy presented here relies on zebra-finch--like consistency of song.

\appendix

\section{Resources}
\label{sec:resources}

\begin{description}
  \item[Song alignment:] Last we checked, Jeff Markowitz's song
    alignment software could be found at \\
    {\tt https://github.com/jmarkow}.

    \item[Training the neural network:] Our \marginpar{Fix the
      location! If going live, I should reorganise my repository, and
      also clean up the code.}  implementation of the syllable
      detector training code is available under the GNU GPL at:
      \\ {\tt https://github.com/bwpearre/}

    \item[Runtime:] The Swift implementation for executing the trained
      network:\\
      {\tt https://github.com/nathanntg/syllable-detector}
\end{description}

\bibliographystyle{plainnat}
\bibliography{rl}

\end{document}

